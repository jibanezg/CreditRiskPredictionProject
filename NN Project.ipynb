{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 74960/119999 [=================>............] - ETA: 3133s - loss: 0.3869 - acc: 0.9320"
     ]
    }
   ],
   "source": [
    "import keras as ks\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm, svd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras import optimizers\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "#Random initialization\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "#function definition\n",
    "def pcaAnalysis(X, lmbda=.01, tol=1e-3,maxiter=100, verbose=True):\n",
    "    \n",
    "    Y = X\n",
    "    norm_two = norm(Y.ravel(), 2)\n",
    "    norm_inf = norm(Y.ravel(), np.inf) / lmbda\n",
    "    dual_norm = np.max([norm_two, norm_inf])\n",
    "    Y = Y / dual_norm\n",
    "    A = np.zeros(Y.shape)\n",
    "    E = np.zeros(Y.shape)\n",
    "    dnorm = norm(X, 'fro')\n",
    "    mu = 1.25 / norm_two\n",
    "    rho = 1.5\n",
    "    sv = 10.\n",
    "    n = Y.shape[0]\n",
    "    itr = 0\n",
    "    while True:\n",
    "        Eraw = X - A + (1 / mu) * Y\n",
    "        Eupdate = np.maximum(Eraw - lmbda / mu, 0) + np.minimum(Eraw + lmbda / mu, 0)\n",
    "        U, S, V = svd(X - Eupdate + (1 / mu) * Y, full_matrices=False)\n",
    "        svp = (S > 1 / mu).shape[0]\n",
    "        if svp < sv:\n",
    "            sv = np.min([svp + 1, n])\n",
    "        else:\n",
    "            sv = np.min([svp + round(.05 * n), n])\n",
    "        Aupdate = np.dot(np.dot(U[:, :svp], np.diag(S[:svp] - 1 / mu)), V[:svp, :])\n",
    "        A = Aupdate\n",
    "        E = Eupdate\n",
    "        Z = X - A - E\n",
    "        Y = Y + mu * Z\n",
    "        mu = np.min([mu * rho, mu * 1e7])\n",
    "        itr += 1\n",
    "        if ((norm(Z, 'fro') / dnorm) < tol) or (itr >= maxiter):\n",
    "            break\n",
    "    if verbose:\n",
    "        print(\"Finished at iteration %d\" % (itr))  \n",
    "    return A, E\n",
    "\n",
    "def neural_network():\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.5, input_shape=(10,)))\n",
    "    model.add(Dense(12, input_dim=10, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(14000, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(7000, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    #model.fit(X_train, y_train, epochs=50, batch_size=10)\n",
    "    \n",
    "    return model\n",
    "\n",
    "dataTrain = pd.read_csv('cs-training.csv').drop('Unnamed: 0', axis = 1)\n",
    "dataTest = pd.read_csv('cs-test.csv').drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "targetTrain = dataTrain['SeriousDlqin2yrs']\n",
    "targetTest = dataTest['SeriousDlqin2yrs']\n",
    "\n",
    "dataTrain = dataTrain.drop(['SeriousDlqin2yrs'],axis=1)\n",
    "dataTest = dataTest.drop(['SeriousDlqin2yrs'],axis=1)\n",
    "\n",
    "#Preprocessing\n",
    "\n",
    "dataTrain = dataTrain.fillna(0.0)\n",
    "dataTest = dataTest.fillna(0.0)\n",
    "#scaler = StandardScaler()\n",
    "#dataTrainNormalized = scaler.fit_transform(dataTrain)\n",
    "#dataTestNormalized = scaler.fit_transform(dataTest)\n",
    "\n",
    "#PCA\n",
    "#dataTrainPCA = np.array(dataTrainNormalized)\n",
    "#dataTestPCA = np.array(dataTestNormalized)\n",
    "\n",
    "#pca = PCA(n_components=8)\n",
    "#pca.fit(dataTrainPCA)\n",
    "#PCA(copy=True, iterated_power='auto', n_components=1, random_state=None,\n",
    "#  svd_solver='auto', tol=0.0, whiten=False)\n",
    "\n",
    "#print(pca.transform(dataTrainPCA))\n",
    "#print(dataTrainPCA)\n",
    "#print(dataTestPCA)\n",
    "#print(np.shape(dataTestPCA))\n",
    "#print(np.shape(dataTrainPCA))\n",
    "#sz = 8\n",
    "#C, D = pcaAnalysis(dataTrainPCA[:,:sz])\n",
    "#dataTrainPCA = C+D\n",
    "#C, D = pcaAnalysis(dataTestPCA[:,:sz])\n",
    "#dataTestPCA = C+D\n",
    "#print(np.shape(dataTestPCA))\n",
    "#print(np.shape(dataTrainPCA))\n",
    "#print(np.shape(dataTestPCA))\n",
    "#print(np.shape(dataTestPCA))\n",
    "\n",
    "#Cross validation data split\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(dataTrainNormalized, targetTrain, test_size=0.4, random_state=0)\n",
    "\n",
    "#K fold validation\n",
    "#from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import model_selection\n",
    "\n",
    "#kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "# Neural Net Dropout\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=neural_network, epochs=30, batch_size=16, verbose=1)))\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "\n",
    "#Evaluation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "scores = cross_val_score(pipeline, dataTrain, targetTrain, cv=kfold)\n",
    "#scores = model.evaluate(X_train, y_train)\n",
    "conf_mat = confusion_matrix(targetTrain,scores)\n",
    "print(conf_mat)\n",
    "#print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
